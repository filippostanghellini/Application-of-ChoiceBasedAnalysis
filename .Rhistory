# Visualizza le prime righe
head(choice_data, 30)
# Controllo con barplot dei rispondenti per vedere che non ci siano bug nella conversione
barplot(table(choice_data$resp.id),
main = "Frequenza per Respondent",
xlab = "resp.id",
ylab = "Frequenza",
las = 2)
# load library for fitting multinomial logit models
library(dfidx)
library(mlogit)
# import the data about the choice_data Survey for conjoint analysis
choice_data <- read.csv("data/Choice_Data_Converted.csv", sep=";")
head(choice_data)
# see some descriptive statistics
summary(choice_data)
xtabs(choice ~ spec, data=choice_data)
xtabs(choice ~ vel, data=choice_data)
xtabs(choice ~ qual, data=choice_data)
xtabs(choice ~ priv, data=choice_data)
xtabs(choice ~ cost, data=choice_data)
# recode some variables
choice_data$spec <- factor(choice_data$spec, levels=c("Assistente","Codice","Content")) # change order of categories
choice_data$vel <- factor(choice_data$vel, levels=c("Lento","Veloce")) # change order of categories
choice_data$qual <- factor(choice_data$qual, levels=c("Sufficente","Ottimale")) # change order of categories
choice_data$priv <- factor(choice_data$priv, levels=c("Bassa","Alta")) # change order of categories
choice_data$cost <- as.factor(choice_data$cost) # convert the variable as qualitative
# 1) Create ID
choice_data$choice_id <- with(choice_data, paste(resp.id, ques, sep = "_"))
# 2) dfix for choice_data
choice_data.mlogit <- dfidx(
choice_data,
idx    = list(c("choice_id", "resp.id"), "alt"),
choice = "choice",
shape  = "long"
)
m1 <- mlogit(choice ~ spec + vel + qual + priv + cost, data = choice_data.mlogit)
summary(m1)
# Script per convertire i dati dal Google Form al formato per Choice-Based Conjoint Analysis
library(tidyverse)
# Funzione per convertire i dati del questionario al formato choice-based
convert_survey_to_choice_format <- function(input_file, output_file) {
# Leggi il file CSV del questionario
survey_data <- read.csv(input_file, sep = ",", header = TRUE, check.names = FALSE)
# Rimuovi le righe dove la risposta alla SCELTA 5 è "Codice - Lento - Sufficiente - Alta - 25€"
# La colonna SCELTA 5 è la nona colonna (indice 9)
choice5_col <- "Specializzazione | Velocità | Qualità | Privacy | Costo  (SCELTA 5)"
invalid_response <- "Codice - Lento - Sufficiente - Alta - 25€"
rows_before <- nrow(survey_data)
survey_data <- survey_data[survey_data[[choice5_col]] != invalid_response, ]
rows_removed <- rows_before - nrow(survey_data)
if (rows_removed > 0) {
cat("Rimosse", rows_removed, "righe con risposta non valida alla SCELTA 5\n")
}
# Numero di choice set (colonne con le scelte - escludi le prime 4 colonne demografiche)
choice_columns <- 5:ncol(survey_data)
n_sets <- length(choice_columns)
# Definizione del design (10 set, 3 alternative ciascuno)
design <- data.frame(
set = c(rep(1, 3), rep(2, 3), rep(3, 3), rep(4, 3), rep(5, 3),
rep(6, 3), rep(7, 3), rep(8, 3), rep(9, 3), rep(10, 3)),
alt = rep(c("alt1", "alt2", "alt3"), 10),
spec = c("Assistente", "Codice", "Content",
"Content", "Assistente", "Codice",
"Codice", "Content", "Assistente",
"Assistente", "Codice", "Content",
"Codice", "Assistente", "Codice",
"Codice", "Content", "Assistente",
"Codice", "Assistente", "Content",
"Codice", "Content", "Assistente",
"Codice", "Assistente", "Content",
"Assistente", "Content", "Content"),
vel = c("Lento", "Lento", "Veloce",
"Veloce", "Veloce", "Lento",
"Lento", "Lento", "Veloce",
"Lento", "Veloce", "Veloce",
"Lento", "Lento", "Veloce",
"Lento", "Veloce", "Veloce",
"Lento", "Lento", "Veloce",
"Veloce", "Lento", "Veloce",
"Veloce", "Lento", "Lento",
"Veloce", "Lento", "Lento"),
qual = c("Ottimale", "Sufficente", "Ottimale",
"Ottimale", "Ottimale", "Sufficente",
"Sufficente", "Ottimale", "Sufficente",
"Sufficente", "Sufficente", "Ottimale",
"Sufficente", "Sufficente", "Ottimale",
"Ottimale", "Ottimale", "Sufficente",
"Ottimale", "Ottimale", "Sufficente",
"Ottimale", "Ottimale", "Sufficente",
"Sufficente", "Ottimale", "Sufficente",
"Sufficente", "Ottimale", "Sufficente"),
priv = c("Bassa", "Alta", "Bassa",
"Bassa", "Bassa", "Alta",
"Bassa", "Alta", "Alta",
"Alta", "Bassa", "Alta",
"Alta", "Bassa", "Alta",
"Bassa", "Bassa", "Alta",
"Bassa", "Alta", "Alta",
"Alta", "Alta", "Bassa",
"Bassa", "Bassa", "Alta",
"Bassa", "Bassa", "Alta"),
cost = c(25, 20, 15,
25, 20, 15,
20, 15, 25,
20, 15, 25,
25, 15, 20,
15, 20, 25,
25, 25, 20,
25, 20, 15,
25, 20, 15,
15, 20, 25)
)
# Mappa per convertire le risposte del form alle alternative
create_choice_map <- function(spec, vel, qual, priv, cost) {
# Converti i nomi come appaiono nel form
spec_form <- ifelse(spec == "Content", "Prod. di contenuti",
ifelse(spec == "Assistente", "Assistente",
ifelse(spec == "Codice", "Codice", spec)))
qual_form <- ifelse(qual == "Sufficente", "Sufficiente", qual)
paste(spec_form, vel, qual_form, priv, paste0(cost, "€"), sep = " - ")
}
# Crea una mappa per identificare le alternative
design$response_text <- create_choice_map(
design$spec,
design$vel,
design$qual,
design$priv,
design$cost
)
# Inizializza il dataframe finale
choice_data <- data.frame()
# Per ogni rispondente
for (resp_id in 1:nrow(survey_data)) {
# Per ogni set di scelta
for (set_id in 1:n_sets) {
# Estrai la risposta per questo set
response <- survey_data[resp_id, choice_columns[set_id]]
# Trova le 3 alternative per questo set
set_alternatives <- design[design$set == set_id, ]
# Per ogni alternativa nel set
for (alt_num in 1:3) {
alt_data <- set_alternatives[alt_num, ]
# Determina se questa alternativa è stata scelta
# Normalizza la risposta per gestire variazioni di formato (spazi multipli, etc)
response_normalized <- gsub("\\s+", " ", trimws(as.character(response)))
alt_text_normalized <- gsub("\\s+", " ", trimws(alt_data$response_text))
choice <- ifelse(response_normalized == alt_text_normalized, 1, 0)
# Crea la riga per questo record
row_data <- data.frame(
resp.id = resp_id,
ques = set_id,
alt = alt_data$alt,
spec = alt_data$spec,
vel = alt_data$vel,
qual = alt_data$qual,
priv = alt_data$priv,
cost = alt_data$cost,
choice = choice
)
choice_data <- rbind(choice_data, row_data)
}
}
}
# Scrivi il file di output
write.csv2(choice_data, output_file, row.names = FALSE, quote = FALSE)
# Statistiche per il log
cat("Conversione completata!\n")
cat("Numero di rispondenti:", nrow(survey_data), "\n")
cat("Numero di set di scelta:", n_sets, "\n")
cat("Numero totale di osservazioni:", nrow(choice_data), "\n")
# Verifica delle scelte per rispondente
choices_per_resp <- choice_data %>%
filter(choice == 1) %>%
group_by(resp.id) %>%
summarise(n_choices = n())
cat("\nRiepilogo scelte per rispondente:\n")
print(summary(choices_per_resp$n_choices))
return(choice_data)
}
# Esegui la conversione
# input_file da sovrascrivere con il file scaricato dal google form
# output_file è il file di destinazione (viene sovrascrtitto ogni volta)
choice_data <- convert_survey_to_choice_format(
input_file = "raw_data/Questionario Lab (Risposte) - Risposte del modulo 1.csv",
output_file = "data/Choice_Data_Converted.csv"
)
# Visualizza le prime righe
head(choice_data, 30)
# Controllo con barplot dei rispondenti per vedere che non ci siano bug nella conversione
barplot(table(choice_data$resp.id),
main = "Frequenza per Respondent",
xlab = "resp.id",
ylab = "Frequenza",
las = 2)
# load library for fitting multinomial logit models
library(dfidx)
library(mlogit)
# import the data about the choice_data Survey for conjoint analysis
choice_data <- read.csv("data/Choice_Data_Converted.csv", sep=";")
head(choice_data)
# see some descriptive statistics
summary(choice_data)
xtabs(choice ~ spec, data=choice_data)
xtabs(choice ~ vel, data=choice_data)
xtabs(choice ~ qual, data=choice_data)
xtabs(choice ~ priv, data=choice_data)
xtabs(choice ~ cost, data=choice_data)
# recode some variables
choice_data$spec <- factor(choice_data$spec, levels=c("Assistente","Codice","Content")) # change order of categories
choice_data$vel <- factor(choice_data$vel, levels=c("Lento","Veloce")) # change order of categories
choice_data$qual <- factor(choice_data$qual, levels=c("Sufficente","Ottimale")) # change order of categories
choice_data$priv <- factor(choice_data$priv, levels=c("Bassa","Alta")) # change order of categories
choice_data$cost <- as.factor(choice_data$cost) # convert the variable as qualitative
# 1) Create ID
choice_data$choice_id <- with(choice_data, paste(resp.id, ques, sep = "_"))
# 2) dfix for choice_data
choice_data.mlogit <- dfidx(
choice_data,
idx    = list(c("choice_id", "resp.id"), "alt"),
choice = "choice",
shape  = "long"
)
m1 <- mlogit(choice ~ spec + vel + qual + priv + cost, data = choice_data.mlogit)
summary(m1)
# Fit the model without intercept parameters
m2 <- mlogit(choice ~ spec + vel + qual + priv + cost | -1, data = choice_data.mlogit)
summary(m2)
# Test the restriction on the intercepts by comparing the two models
# through a likelihood ratio test
lrtest(m2, m1)
# Fit the model without intercept parameters and with price as a quantitative variable
m3 <- mlogit(choice ~ spec + vel + qual + priv
+ as.numeric(as.character(cost)) | -1, data = choice_data.mlogit)
summary(m3)
lrtest(m3, m2)
# Compute the willingness to pay for privacy alta
#WTP privacy
-coef(m3)["privAlta"]/(coef(m3)["as.numeric(as.character(cost))"])
#WTP codice
-coef(m3)["specCodice"]/(coef(m3)["as.numeric(as.character(cost))"])
#WTP content
-coef(m3)["specContent"]/(coef(m3)["as.numeric(as.character(cost))"])
#WTP veloce
-coef(m3)["velVeloce"]/(coef(m3)["as.numeric(as.character(cost))"])
#WTP quality
-coef(m3)["qualOttimale"]/(coef(m3)["as.numeric(as.character(cost))"])
# Delta method for WTP
# It allow us to calculate standard error and CI for each WTP
library(car)
attributes <- c("specCodice", "specContent", "velVeloce", "qualOttimale", "privAlta")
wtp_table <- data.frame()
for(attr in attributes) {
formula_str <- paste0("-", attr, " / `as.numeric(as.character(cost))`")
dm <- deltaMethod(m3, formula_str)
wtp_table <- rbind(wtp_table, data.frame(
Attributo = attr,
WTP = round(dm$Estimate, 2),
SE = round(dm$SE, 2),
CI_Lower = round(dm$`2.5 %`, 2),
CI_Upper = round(dm$`97.5 %`, 2)
))
}
print(wtp_table)
# Test the restriction on the intercepts by comparing the two models
# through a likelihood ratio test
lrtest(m2, m1)
# Fit the model without intercept parameters and with price as a quantitative variable
m3 <- mlogit(choice ~ spec + vel + qual + priv
+ as.numeric(as.character(cost)) | -1, data = choice_data.mlogit)
summary(m3)
lrtest(m3, m2)
# Compute the willingness to pay for privacy alta
#WTP privacy
-coef(m3)["privAlta"]/(coef(m3)["as.numeric(as.character(cost))"])
#WTP codice
-coef(m3)["specCodice"]/(coef(m3)["as.numeric(as.character(cost))"])
#WTP content
-coef(m3)["specContent"]/(coef(m3)["as.numeric(as.character(cost))"])
#WTP veloce
-coef(m3)["velVeloce"]/(coef(m3)["as.numeric(as.character(cost))"])
#WTP quality
-coef(m3)["qualOttimale"]/(coef(m3)["as.numeric(as.character(cost))"])
# Delta method for WTP
# It allow us to calculate standard error and CI for each WTP
library(car)
attributes <- c("specCodice", "specContent", "velVeloce", "qualOttimale", "privAlta")
wtp_table <- data.frame()
for(attr in attributes) {
formula_str <- paste0("-", attr, " / `as.numeric(as.character(cost))`")
dm <- deltaMethod(m3, formula_str)
wtp_table <- rbind(wtp_table, data.frame(
Attributo = attr,
WTP = round(dm$Estimate, 2),
SE = round(dm$SE, 2),
CI_Lower = round(dm$`2.5 %`, 2),
CI_Upper = round(dm$`97.5 %`, 2)
))
}
print(wtp_table)
# Simulate preference shares using the "predict.mnl" function
# Define the function
predict.mnl <- function(model, data) {
# Function for predicting preference shares from a MNL model
# model: mlogit object returned by mlogit()
# data: a data frame containing the set of designs for which you want to
# predict shares.  Same format at the data used to estimate model.
data.model <- model.matrix(update(model$formula, 0 ~ .), data = data)[,-1]
logitUtility <- data.model%*%model$coef
share <- exp(logitUtility)/sum(exp(logitUtility))
cbind(share, data)
}
# In order to use "predict.mnl", you need to define a data frame containing the set of designs
# for which you want to predict the preference shares.
# One way to do this is to create the full set of possible designs
# using expand.grid() and select the designs we want by row number
attributes <- list(spec=names(table(choice_data.mlogit$spec)),
vel=names(table(choice_data.mlogit$vel)),
qual=names(table(choice_data.mlogit$qual)),
priv=names(table(choice_data.mlogit$priv)),
cost=names(table(choice_data.mlogit$cost)))
allDesign <- expand.grid(attributes)
allDesign #all possible design
# we choose a reasonable and realistic subset (where the first row indicates our design), such as
new.data <- allDesign[c(2, 32, 28, 67, 6, 69), ]
new.data
# We then pass these designs to predict.mnl() to determine what customers
# would choose if they had to pick among these six choice_data alternatives:
predict.mnl(m3, new.data) # using m3 specification, price numerica
# Compute and plot preference share sensitivity
# Producing a sensitivity chart using R is relatively simple: we just need to loop through all
# the attribute levels, compute a preference share prediction, and save the predicted preference share for
# the target design. The "sensitivity.mnl" function does that.
sensitivity.mnl <- function(model, attrib, base.data, competitor.data) {
# Function for creating data for a preference share-sensitivity chart
# model: mlogit object returned by mlogit() function
# attrib: list of vectors with attribute levels to be used in sensitivity
# base.data: data frame containing baseline design of target product
# competitor.data: data frame contining design of competitive set
data <- rbind(base.data, competitor.data)
base.share <- predict.mnl(model, data)[1,1]
share <- NULL
for (a in seq_along(attrib)) {
for (i in attrib[[a]]) {
data[1,] <- base.data
data[1,a] <- i
share <- c(share, predict.mnl(model, data)[1,1])
}
}
data.frame(level=unlist(attrib), share=share, increase=share-base.share)
}
base.data <- new.data[1,]
competitor.data <- new.data[-1,]
(tradeoff <- sensitivity.mnl(m2, attributes, base.data, competitor.data))
barplot(tradeoff$increase, horiz=FALSE, names.arg=tradeoff$level,
ylab="Change in Share for the Planned Product Design",
ylim=c(-0.1,0.11))
grid(nx=NA, ny=NULL)
# Colored barplot for our future implementation
cols <- rep("gray", length(tradeoff$level))
cols[tradeoff$level == "Alta"] <- "red"
barplot(tradeoff$increase,
horiz = FALSE,
names.arg = tradeoff$level,
ylab = "Change in Share for the Planned Product Design",
ylim = c(-0.1, 0.11),
col = cols)
grid(nx = NA, ny = NULL)
new.data_new <- allDesign[c(14, 32, 28, 67, 6, 69), ]
new.data_new
predict.mnl(m3, new.data_new)
# Fit a mixed MNL model
# The statistical term for coefficients that vary across respondents (or customers) is
# random coefficients or random effects. To estimate a multinomial
# logit model with random coefficients using "mlogit", we define a vector indicating
# which coefficients should vary across customers. "mlogit" requires a character
# vector the same length as the coefficient vector with a letter code indicating what
# distribution the random coefficients should follow across the respondents: "n" for
# normal, "l" for log normal, "t" for truncated normal, and "u" for uniform. For this
# analysis, we assume that all the coefficients are normally distributed across the population
# and call our vector "m2.rpar".
m2.rpar <- rep("n", length=length(m2$coef))
names(m2.rpar) <- names(m2$coef)
m2.rpar
# We pass this vector to mlogit as the rpar parameter, which is short for "random
# parameters". In addition, we tell mlogit that we have multiple choice observations
# for each respondent (panel=TRUE) and whether we want to allow the random
# parameters to be correlated with each other. For this first run, we assume that we do
# not want random parameters to be correlated (correlation=FALSE), a setting
# we reconsider below.
m2.mixed <- mlogit(choice ~ spec + vel + qual + priv + cost | -1,
data = choice_data.mlogit,
panel=TRUE, rpar = m2.rpar, correlation = FALSE)
summary(m2.mixed)
# We can get a visual summary of the distribution of random effects and hence of the level of heterogeneity
plot(m2.mixed)
# We can extract the distribution of specific random effects using the function rpar()
specCodice.distr <- rpar(m2.mixed, "specCodice")
summary(specCodice.distr)
mean(specCodice.distr)
med(specCodice.distr)
plot(specCodice.distr)
# We can add that the random coefficients can be correlated
# This is easily done by including "correlations = TRUE"
# as an option in the call to mlogit or by using the update function
# provided by mlogit
m2.mixed2 <- update(m2.mixed, correlation = TRUE)
summary(m2.mixed2)
# To get a better sense of the strength of the association among random coefficients,
# we can extract the covariance matrix using "cov.mlogit"
# and then convert it to a correlation matrix using "cov2cor" from base R.
cov2cor(cov.mlogit(m2.mixed2))
# Calculate correlation matrix
cor_mat <- cov2cor(cov.mlogit(m2.mixed2))
library(ggplot2)
library(reshape2)
cor_mat <- cov2cor(cov.mlogit(m2.mixed2))
cor_long <- melt(cor_mat)
ggplot(cor_long, aes(Var1, Var2, fill = value)) +
geom_tile() +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1, 1)) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(x = "", y = "", fill = "Correlation")
# We can also obtain the standard errors of the correlations among random effects,
# and hence perform significance test
summary(vcov(m2.mixed2, what = "rpar", type = "cor"))
# We may restrict the correlation to only random parameters with significant association
m2.mixed3 <- update(m2.mixed2, correlation = c("specCodice", "specContent", "privAlta", "velVeloce", "cost20", "cost25"))
# The significant presence of random coefficients and their correlation
# can be further investigated using the ML tests, such as the ML ratio test
lrtest(m2, m2.mixed) #Fixed effects vs. uncorrelated random effects
lrtest(m2.mixed, m2.mixed2) #Uncorrelated random effects vs. all correlated random effects
lrtest(m2.mixed3, m2.mixed2) #Partially correlated random effects vs. all correlated random effects
library(MASS)
predict.mixed.mnl <- function(model, data, nresp=1000) {
# Function for predicting shares from a mixed MNL model
# model: mlogit object returned by mlogit()
# data: a data frame containing the set of designs for which you want to
#       predict shares. Same format at the data used to estimate model.
# Note that this code assumes all model parameters are random
data.model <- model.matrix(update(model$formula, 0 ~ .), data = data)[,-1]
coef.Sigma <- cov.mlogit(model)
coef.mu <- model$coef[1:dim(coef.Sigma)[1]]
draws <- mvrnorm(n=nresp, coef.mu, coef.Sigma)
shares <- matrix(NA, nrow=nresp, ncol=nrow(data))
for (i in 1:nresp) {
utility <- data.model%*%draws[i,]
share = exp(utility)/sum(exp(utility))
shares[i,] <- share
}
cbind(colMeans(shares), data)
}
set.seed(1111)
predict.mixed.mnl(m2.mixed2, data=new.data)
predict.mixed.mnl(m2.mixed2, data=new.data_new) # prediciton with our new product
# We can also obtain the standard errors of the correlations among random effects,
# and hence perform significance test
summary(vcov(m2.mixed2, what = "rpar", type = "cor"))
lrtest(m3, m2)
# Fit a mixed MNL model
# The statistical term for coefficients that vary across respondents (or customers) is
# random coefficients or random effects. To estimate a multinomial
# logit model with random coefficients using "mlogit", we define a vector indicating
# which coefficients should vary across customers. "mlogit" requires a character
# vector the same length as the coefficient vector with a letter code indicating what
# distribution the random coefficients should follow across the respondents: "n" for
# normal, "l" for log normal, "t" for truncated normal, and "u" for uniform. For this
# analysis, we assume that all the coefficients are normally distributed across the population
# and call our vector "m2.rpar".
m2.rpar <- rep("n", length=length(m2$coef))
names(m2.rpar) <- names(m2$coef)
m2.rpar
# We pass this vector to mlogit as the rpar parameter, which is short for "random
# parameters". In addition, we tell mlogit that we have multiple choice observations
# for each respondent (panel=TRUE) and whether we want to allow the random
# parameters to be correlated with each other. For this first run, we assume that we do
# not want random parameters to be correlated (correlation=FALSE), a setting
# we reconsider below.
m2.mixed <- mlogit(choice ~ spec + vel + qual + priv + cost | -1,
data = choice_data.mlogit,
panel=TRUE, rpar = m2.rpar, correlation = FALSE)
summary(m2.mixed)
# We can get a visual summary of the distribution of random effects and hence of the level of heterogeneity
plot(m2.mixed)
# We can extract the distribution of specific random effects using the function rpar()
specCodice.distr <- rpar(m2.mixed, "specCodice")
summary(specCodice.distr)
mean(specCodice.distr)
med(specCodice.distr)
plot(specCodice.distr)
# To get a better sense of the strength of the association among random coefficients,
# we can extract the covariance matrix using "cov.mlogit"
# and then convert it to a correlation matrix using "cov2cor" from base R.
cov2cor(cov.mlogit(m2.mixed2))
# Calculate correlation matrix
cor_mat <- cov2cor(cov.mlogit(m2.mixed2))
library(ggplot2)
library(reshape2)
cor_mat <- cov2cor(cov.mlogit(m2.mixed2))
cor_long <- melt(cor_mat)
ggplot(cor_long, aes(Var1, Var2, fill = value)) +
geom_tile() +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1, 1)) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(x = "", y = "", fill = "Correlation")
# We can also obtain the standard errors of the correlations among random effects,
# and hence perform significance test
summary(vcov(m2.mixed2, what = "rpar", type = "cor"))
